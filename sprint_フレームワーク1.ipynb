{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sprint_フレームワーク1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1N50xmQWUOFmE6e0bRLXdo0nmRxd29GOG",
      "authorship_tag": "ABX9TyMchs40fhTOT5yNyo/V/zik",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YusukeAoki57/diveintocode-ml/blob/master/sprint_%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E3%83%AF%E3%83%BC%E3%82%AF1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE1Gv2xEM2T_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "208af1d9-2f51-4b8b-ce71-7352cc997b13"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print(tf.__version__)\r\n",
        "!pip install --upgrade tensorflow==1.14.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n",
            "Requirement already up-to-date: tensorflow==1.14.0 in /usr/local/lib/python3.7/dist-packages (1.14.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (54.0.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.2)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjfrbAc2OQ8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07846333-f149-4ccf-bcab-968e5118312b"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA96SLAJSss9"
      },
      "source": [
        "#問題1\r\n",
        "#①エポックを更新\r\n",
        "#②ロスを最小化"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mLB9S8YVThV"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.datasets import load_iris\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwq-2GGnVkb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d293e189-9522-44fb-9633-af9bac21cdab"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNgjpICt_INm"
      },
      "source": [
        "df= pd.read_csv('drive/My Drive/DIVE INTO CODE/0_課題/3月/week3/Iris.csv',dtype = None)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDiozXdTWAVt"
      },
      "source": [
        "#問題2"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsPysa_DWKsK"
      },
      "source": [
        "#はじめに使う箱が用意され、下位でwithで実行した際に実際のデータが箱に入れられて実行されている"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIxgQgdLYbuR"
      },
      "source": [
        "#問題3"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AedEA9bAifk"
      },
      "source": [
        "#df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\r\n",
        "y = df[\"Species\"]\r\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\r\n",
        "# NumPy 配列に変換\r\n",
        "X = np.array(X)\r\n",
        "y = np.array(y)\r\n",
        "# ラベルを数値に変換\r\n",
        "y[y == \"Iris-versicolor\"] = 0\r\n",
        "y[y == \"Iris-virginica\"] = 1\r\n",
        "y[y == \"Iris-setosa\"] = 2\r\n",
        "y = y.reshape(-1, 1) == np.arange(3)\r\n",
        "#y = y.astype(np.int64)[:, np.newaxis]\r\n",
        "# trainとtestに分割\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\r\n",
        "# さらにtrainとvalに分割\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD3Loh4vAjiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f52c03ef-c1c8-431c-d06a-62706162c9f6"
      },
      "source": [
        "class GetMiniBatch:\r\n",
        "    \"\"\"\r\n",
        "    ミニバッチを取得するイテレータ\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\r\n",
        "      訓練データ\r\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\r\n",
        "      正解値\r\n",
        "    batch_size : int\r\n",
        "      バッチサイズ\r\n",
        "    seed : int\r\n",
        "      NumPyの乱数のシード\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\r\n",
        "        self.batch_size = batch_size\r\n",
        "        np.random.seed(seed)\r\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\r\n",
        "        self.X = X[shuffle_index]\r\n",
        "        self.y = y[shuffle_index]\r\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\r\n",
        "    def __len__(self):\r\n",
        "        return self._stop\r\n",
        "    def __getitem__(self,item):\r\n",
        "        p0 = item*self.batch_size\r\n",
        "        p1 = item*self.batch_size + self.batch_size\r\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \r\n",
        "    def __iter__(self):\r\n",
        "        self._counter = 0\r\n",
        "        return self\r\n",
        "    def __next__(self):\r\n",
        "        if self._counter >= self._stop:\r\n",
        "            raise StopIteration()\r\n",
        "        p0 = self._counter*self.batch_size\r\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\r\n",
        "        self._counter += 1\r\n",
        "        return self.X[p0:p1], self.y[p0:p1]\r\n",
        "# ハイパーパラメータの設定\r\n",
        "learning_rate = 0.001\r\n",
        "batch_size = 10\r\n",
        "num_epochs = 100\r\n",
        "n_hidden1 = 50\r\n",
        "n_hidden2 = 100\r\n",
        "n_input = X_train.shape[1]\r\n",
        "n_samples = X_train.shape[0]\r\n",
        "n_classes = 3\r\n",
        "# 計算グラフに渡す引数の形を決める\r\n",
        "X = tf.placeholder(\"float\", [None, n_input])\r\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\r\n",
        "# trainのミニバッチイテレータ\r\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\r\n",
        "def example_net(x):\r\n",
        "    \"\"\"\r\n",
        "    単純な3層ニューラルネットワーク\r\n",
        "    \"\"\"\r\n",
        "    tf.random.set_random_seed(0)\r\n",
        "    # 重みとバイアスの宣言\r\n",
        "    weights = {\r\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\r\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\r\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\r\n",
        "    }\r\n",
        "    biases = {\r\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\r\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\r\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\r\n",
        "    }\r\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\r\n",
        "    layer_1 = tf.nn.relu(layer_1)\r\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\r\n",
        "    layer_2 = tf.nn.relu(layer_2)\r\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\r\n",
        "    return layer_output\r\n",
        "# ネットワーク構造の読み込み                               \r\n",
        "logits = example_net(X)\r\n",
        "# 目的関数\r\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\r\n",
        "# 最適化手法\r\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n",
        "train_op = optimizer.minimize(loss_op)\r\n",
        "# 推定結果\r\n",
        "correct_pred = tf.equal(tf.argmax(Y,1), tf.argmax(logits,1))\r\n",
        "# 指標値計算\r\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n",
        "# variableの初期化\r\n",
        "init = tf.global_variables_initializer()\r\n",
        "\r\n",
        "# 計算グラフの実行\r\n",
        "with tf.Session() as sess:\r\n",
        "    sess.run(init)\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        # エポックごとにループ\r\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\r\n",
        "        total_loss = 0\r\n",
        "        total_acc = 0\r\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\r\n",
        "            # ミニバッチごとにループ\r\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\r\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\r\n",
        "            total_loss += loss\r\n",
        "        total_loss /= n_samples\r\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\r\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\r\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\r\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss : 4.9528, val_loss : 23.9570, acc : 0.083\n",
            "Epoch 1, loss : 1.5035, val_loss : 9.1359, acc : 0.583\n",
            "Epoch 2, loss : 0.5222, val_loss : 5.9294, acc : 0.625\n",
            "Epoch 3, loss : 0.3899, val_loss : 6.3386, acc : 0.667\n",
            "Epoch 4, loss : 0.2103, val_loss : 3.8333, acc : 0.708\n",
            "Epoch 5, loss : 0.1628, val_loss : 4.3559, acc : 0.667\n",
            "Epoch 6, loss : 0.1110, val_loss : 3.3110, acc : 0.750\n",
            "Epoch 7, loss : 0.0933, val_loss : 3.2104, acc : 0.750\n",
            "Epoch 8, loss : 0.0632, val_loss : 2.6237, acc : 0.750\n",
            "Epoch 9, loss : 0.0634, val_loss : 2.6110, acc : 0.750\n",
            "Epoch 10, loss : 0.0536, val_loss : 2.5790, acc : 0.750\n",
            "Epoch 11, loss : 0.0472, val_loss : 2.3321, acc : 0.750\n",
            "Epoch 12, loss : 0.0448, val_loss : 2.3494, acc : 0.792\n",
            "Epoch 13, loss : 0.0426, val_loss : 2.1896, acc : 0.750\n",
            "Epoch 14, loss : 0.0425, val_loss : 2.2535, acc : 0.792\n",
            "Epoch 15, loss : 0.0410, val_loss : 2.2152, acc : 0.792\n",
            "Epoch 16, loss : 0.0400, val_loss : 2.1628, acc : 0.792\n",
            "Epoch 17, loss : 0.0392, val_loss : 2.1514, acc : 0.792\n",
            "Epoch 18, loss : 0.0382, val_loss : 2.1057, acc : 0.792\n",
            "Epoch 19, loss : 0.0374, val_loss : 2.0536, acc : 0.833\n",
            "Epoch 20, loss : 0.0366, val_loss : 2.0071, acc : 0.833\n",
            "Epoch 21, loss : 0.0357, val_loss : 1.9509, acc : 0.833\n",
            "Epoch 22, loss : 0.0350, val_loss : 1.8949, acc : 0.833\n",
            "Epoch 23, loss : 0.0342, val_loss : 1.8405, acc : 0.833\n",
            "Epoch 24, loss : 0.0335, val_loss : 1.7855, acc : 0.833\n",
            "Epoch 25, loss : 0.0328, val_loss : 1.7333, acc : 0.833\n",
            "Epoch 26, loss : 0.0321, val_loss : 1.6829, acc : 0.833\n",
            "Epoch 27, loss : 0.0314, val_loss : 1.6352, acc : 0.833\n",
            "Epoch 28, loss : 0.0307, val_loss : 1.5905, acc : 0.833\n",
            "Epoch 29, loss : 0.0300, val_loss : 1.5488, acc : 0.833\n",
            "Epoch 30, loss : 0.0294, val_loss : 1.5102, acc : 0.833\n",
            "Epoch 31, loss : 0.0288, val_loss : 1.4746, acc : 0.833\n",
            "Epoch 32, loss : 0.0281, val_loss : 1.4419, acc : 0.875\n",
            "Epoch 33, loss : 0.0275, val_loss : 1.4119, acc : 0.875\n",
            "Epoch 34, loss : 0.0269, val_loss : 1.3844, acc : 0.875\n",
            "Epoch 35, loss : 0.0263, val_loss : 1.3591, acc : 0.875\n",
            "Epoch 36, loss : 0.0257, val_loss : 1.3359, acc : 0.875\n",
            "Epoch 37, loss : 0.0251, val_loss : 1.3146, acc : 0.875\n",
            "Epoch 38, loss : 0.0245, val_loss : 1.2950, acc : 0.875\n",
            "Epoch 39, loss : 0.0239, val_loss : 1.2772, acc : 0.875\n",
            "Epoch 40, loss : 0.0233, val_loss : 1.2608, acc : 0.875\n",
            "Epoch 41, loss : 0.0227, val_loss : 1.2455, acc : 0.875\n",
            "Epoch 42, loss : 0.0221, val_loss : 1.2315, acc : 0.875\n",
            "Epoch 43, loss : 0.0216, val_loss : 1.2179, acc : 0.875\n",
            "Epoch 44, loss : 0.0210, val_loss : 1.2065, acc : 0.875\n",
            "Epoch 45, loss : 0.0204, val_loss : 1.1942, acc : 0.875\n",
            "Epoch 46, loss : 0.0199, val_loss : 1.1848, acc : 0.875\n",
            "Epoch 47, loss : 0.0193, val_loss : 1.1735, acc : 0.875\n",
            "Epoch 48, loss : 0.0188, val_loss : 1.1655, acc : 0.875\n",
            "Epoch 49, loss : 0.0183, val_loss : 1.1552, acc : 0.875\n",
            "Epoch 50, loss : 0.0178, val_loss : 1.1477, acc : 0.875\n",
            "Epoch 51, loss : 0.0172, val_loss : 1.1385, acc : 0.875\n",
            "Epoch 52, loss : 0.0167, val_loss : 1.1297, acc : 0.875\n",
            "Epoch 53, loss : 0.0163, val_loss : 1.1217, acc : 0.875\n",
            "Epoch 54, loss : 0.0158, val_loss : 1.1106, acc : 0.875\n",
            "Epoch 55, loss : 0.0154, val_loss : 1.1024, acc : 0.875\n",
            "Epoch 56, loss : 0.0149, val_loss : 1.0893, acc : 0.875\n",
            "Epoch 57, loss : 0.0145, val_loss : 1.0791, acc : 0.875\n",
            "Epoch 58, loss : 0.0141, val_loss : 1.0647, acc : 0.875\n",
            "Epoch 59, loss : 0.0137, val_loss : 1.0517, acc : 0.875\n",
            "Epoch 60, loss : 0.0133, val_loss : 1.0366, acc : 0.875\n",
            "Epoch 61, loss : 0.0129, val_loss : 1.0215, acc : 0.875\n",
            "Epoch 62, loss : 0.0126, val_loss : 1.0061, acc : 0.875\n",
            "Epoch 63, loss : 0.0122, val_loss : 0.9903, acc : 0.875\n",
            "Epoch 64, loss : 0.0119, val_loss : 0.9749, acc : 0.917\n",
            "Epoch 65, loss : 0.0115, val_loss : 0.9593, acc : 0.917\n",
            "Epoch 66, loss : 0.0112, val_loss : 0.9445, acc : 0.917\n",
            "Epoch 67, loss : 0.0109, val_loss : 0.9295, acc : 0.917\n",
            "Epoch 68, loss : 0.0106, val_loss : 0.9155, acc : 0.917\n",
            "Epoch 69, loss : 0.0103, val_loss : 0.9012, acc : 0.917\n",
            "Epoch 70, loss : 0.0100, val_loss : 0.8880, acc : 0.917\n",
            "Epoch 71, loss : 0.0097, val_loss : 0.8743, acc : 0.917\n",
            "Epoch 72, loss : 0.0094, val_loss : 0.8619, acc : 0.917\n",
            "Epoch 73, loss : 0.0091, val_loss : 0.8484, acc : 0.917\n",
            "Epoch 74, loss : 0.0088, val_loss : 0.8367, acc : 0.917\n",
            "Epoch 75, loss : 0.0085, val_loss : 0.8234, acc : 0.917\n",
            "Epoch 76, loss : 0.0082, val_loss : 0.8125, acc : 0.917\n",
            "Epoch 77, loss : 0.0079, val_loss : 0.7989, acc : 0.917\n",
            "Epoch 78, loss : 0.0076, val_loss : 0.7888, acc : 0.917\n",
            "Epoch 79, loss : 0.0073, val_loss : 0.7747, acc : 0.917\n",
            "Epoch 80, loss : 0.0071, val_loss : 0.7656, acc : 0.917\n",
            "Epoch 81, loss : 0.0068, val_loss : 0.7506, acc : 0.917\n",
            "Epoch 82, loss : 0.0066, val_loss : 0.7430, acc : 0.917\n",
            "Epoch 83, loss : 0.0062, val_loss : 0.7264, acc : 0.917\n",
            "Epoch 84, loss : 0.0061, val_loss : 0.7212, acc : 0.917\n",
            "Epoch 85, loss : 0.0057, val_loss : 0.7021, acc : 0.917\n",
            "Epoch 86, loss : 0.0056, val_loss : 0.7007, acc : 0.917\n",
            "Epoch 87, loss : 0.0052, val_loss : 0.6775, acc : 0.917\n",
            "Epoch 88, loss : 0.0052, val_loss : 0.6817, acc : 0.917\n",
            "Epoch 89, loss : 0.0047, val_loss : 0.6525, acc : 0.917\n",
            "Epoch 90, loss : 0.0049, val_loss : 0.6646, acc : 0.917\n",
            "Epoch 91, loss : 0.0042, val_loss : 0.6275, acc : 0.917\n",
            "Epoch 92, loss : 0.0045, val_loss : 0.6493, acc : 0.917\n",
            "Epoch 93, loss : 0.0038, val_loss : 0.6041, acc : 0.917\n",
            "Epoch 94, loss : 0.0042, val_loss : 0.6343, acc : 0.917\n",
            "Epoch 95, loss : 0.0034, val_loss : 0.5848, acc : 0.917\n",
            "Epoch 96, loss : 0.0039, val_loss : 0.6176, acc : 0.917\n",
            "Epoch 97, loss : 0.0031, val_loss : 0.5720, acc : 0.917\n",
            "Epoch 98, loss : 0.0035, val_loss : 0.5990, acc : 0.917\n",
            "Epoch 99, loss : 0.0029, val_loss : 0.5657, acc : 0.917\n",
            "test_acc : 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHeOJ2GWArFA"
      },
      "source": [
        "\r\n",
        "#問題4\r\n",
        "df= pd.read_csv('drive/My Drive/DIVE INTO CODE/0_課題/1月/2_week3/授業前課題/data/train.csv',dtype = None)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKVyfmFhdnwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439a1bfc-457c-44f7-a4d6-05dfc2e8298d"
      },
      "source": [
        "x_1=df.loc[:,'GrLivArea']\r\n",
        "x_2=df.loc[:,'YearBuilt']\r\n",
        "\r\n",
        "X=pd.concat([x_1,x_2],axis=1)\r\n",
        "y=df.loc[:,'SalePrice']\r\n",
        "y = y.astype(np.int64)[:, np.newaxis]\r\n",
        "X = np.array(X)\r\n",
        "y = np.array(y)\r\n",
        "# trainとtestに分割\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\r\n",
        "# さらにtrainとvalに分割\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\r\n",
        "print(X_train.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(934, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ6xwgvNebv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02fb42e8-fc60-4e8e-ace2-4037abdb7142"
      },
      "source": [
        "class GetMiniBatch:\r\n",
        "    \"\"\"\r\n",
        "    ミニバッチを取得するイテレータ\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\r\n",
        "      訓練データ\r\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\r\n",
        "      正解値\r\n",
        "    batch_size : int\r\n",
        "      バッチサイズ\r\n",
        "    seed : int\r\n",
        "      NumPyの乱数のシード\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\r\n",
        "        self.batch_size = batch_size\r\n",
        "        np.random.seed(seed)\r\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\r\n",
        "        self.X = X[shuffle_index]\r\n",
        "        self.y = y[shuffle_index]\r\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\r\n",
        "    def __len__(self):\r\n",
        "        return self._stop\r\n",
        "    def __getitem__(self,item):\r\n",
        "        p0 = item*self.batch_size\r\n",
        "        p1 = item*self.batch_size + self.batch_size\r\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \r\n",
        "    def __iter__(self):\r\n",
        "        self._counter = 0\r\n",
        "        return self\r\n",
        "    def __next__(self):\r\n",
        "        if self._counter >= self._stop:\r\n",
        "            raise StopIteration()\r\n",
        "        p0 = self._counter*self.batch_size\r\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\r\n",
        "        self._counter += 1\r\n",
        "        return self.X[p0:p1], self.y[p0:p1]\r\n",
        "# ハイパーパラメータの設定\r\n",
        "learning_rate = 0.001\r\n",
        "batch_size = 10\r\n",
        "num_epochs = 100\r\n",
        "n_hidden1 = 50\r\n",
        "n_hidden2 = 100\r\n",
        "n_input = X_train.shape[1]\r\n",
        "n_samples = X_train.shape[0]\r\n",
        "n_classes = 1\r\n",
        "# 計算グラフに渡す引数の形を決める\r\n",
        "X = tf.placeholder(\"float\", [None, n_input])\r\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\r\n",
        "# trainのミニバッチイテレータ\r\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\r\n",
        "def example_net(x):\r\n",
        "    \"\"\"\r\n",
        "    単純な3層ニューラルネットワーク\r\n",
        "    \"\"\"\r\n",
        "    tf.random.set_random_seed(0)\r\n",
        "    # 重みとバイアスの宣言\r\n",
        "    weights = {\r\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\r\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\r\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\r\n",
        "    }\r\n",
        "    biases = {\r\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\r\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\r\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\r\n",
        "    }\r\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\r\n",
        "    \r\n",
        "    layer_1 = tf.nn.relu(layer_1)\r\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\r\n",
        "    layer_2 = tf.nn.relu(layer_2)\r\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\r\n",
        "    return layer_output\r\n",
        "# ネットワーク構造の読み込み                               \r\n",
        "logits = example_net(X)\r\n",
        "\r\n",
        "# 目的関数\r\n",
        "loss_op=tf.reduce_sum(tf.square(logits-Y))\r\n",
        "#loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\r\n",
        "# 最適化手法\r\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n",
        "train_op = optimizer.minimize(loss_op)\r\n",
        "\r\n",
        "# 推定結果\r\n",
        "correct_pred=logits\r\n",
        "print(correct_pred)\r\n",
        "# 指標値計算\r\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n",
        "# variableの初期化\r\n",
        "init = tf.global_variables_initializer()\r\n",
        "\r\n",
        "# 計算グラフの実行\r\n",
        "with tf.Session() as sess:\r\n",
        "    sess.run(init)\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        # エポックごとにループ\r\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\r\n",
        "        total_loss = 0\r\n",
        "        total_acc = 0\r\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\r\n",
        "            # ミニバッチごとにループ\r\n",
        "            #print(mini_batch_x.shape)\r\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\r\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\r\n",
        "            total_loss += loss\r\n",
        "        total_loss /= n_samples\r\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\r\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\r\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\r\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"add_8:0\", shape=(?, 1), dtype=float32)\n",
            "Epoch 0, loss : 6759171098.3126, val_loss : 904482390016.0000, acc : 182545.609\n",
            "Epoch 1, loss : 4208848136.2227, val_loss : 707323166720.0000, acc : 179470.891\n",
            "Epoch 2, loss : 3574662746.4497, val_loss : 619675385856.0000, acc : 176779.797\n",
            "Epoch 3, loss : 3292100707.7687, val_loss : 587433115648.0000, acc : 175766.219\n",
            "Epoch 4, loss : 3164180054.3383, val_loss : 577720942592.0000, acc : 174767.875\n",
            "Epoch 5, loss : 3106836174.9379, val_loss : 577299808256.0000, acc : 174326.484\n",
            "Epoch 6, loss : 3089007625.3191, val_loss : 578146926592.0000, acc : 174141.844\n",
            "Epoch 7, loss : 3081865788.8480, val_loss : 578430763008.0000, acc : 173988.969\n",
            "Epoch 8, loss : 3078232460.3340, val_loss : 578698870784.0000, acc : 173930.984\n",
            "Epoch 9, loss : 3075761448.0171, val_loss : 578878832640.0000, acc : 173903.297\n",
            "Epoch 10, loss : 3074276382.1499, val_loss : 578954461184.0000, acc : 173877.203\n",
            "Epoch 11, loss : 3073123677.7388, val_loss : 578815393792.0000, acc : 173891.656\n",
            "Epoch 12, loss : 3072004753.8158, val_loss : 578630647808.0000, acc : 173914.922\n",
            "Epoch 13, loss : 3071301189.0707, val_loss : 578623504384.0000, acc : 173919.344\n",
            "Epoch 14, loss : 3070804147.8030, val_loss : 578564587520.0000, acc : 173955.578\n",
            "Epoch 15, loss : 3070363446.8180, val_loss : 578611707904.0000, acc : 173942.484\n",
            "Epoch 16, loss : 3070011893.0364, val_loss : 578578087936.0000, acc : 173950.938\n",
            "Epoch 17, loss : 3069805556.4882, val_loss : 578528083968.0000, acc : 173962.000\n",
            "Epoch 18, loss : 3069445060.7966, val_loss : 578484830208.0000, acc : 173967.875\n",
            "Epoch 19, loss : 3069137697.9872, val_loss : 578374336512.0000, acc : 173979.281\n",
            "Epoch 20, loss : 3069130257.5418, val_loss : 578321842176.0000, acc : 173990.375\n",
            "Epoch 21, loss : 3068425165.5675, val_loss : 578260500480.0000, acc : 174003.609\n",
            "Epoch 22, loss : 3068373598.2869, val_loss : 578303164416.0000, acc : 173983.750\n",
            "Epoch 23, loss : 3068198333.1221, val_loss : 578278129664.0000, acc : 173984.797\n",
            "Epoch 24, loss : 3067885572.3854, val_loss : 578250866688.0000, acc : 173993.531\n",
            "Epoch 25, loss : 3067612971.8544, val_loss : 578245885952.0000, acc : 173990.328\n",
            "Epoch 26, loss : 3067432172.8137, val_loss : 578221506560.0000, acc : 173996.859\n",
            "Epoch 27, loss : 3067148274.2955, val_loss : 578134409216.0000, acc : 173993.078\n",
            "Epoch 28, loss : 3066501240.0514, val_loss : 578092269568.0000, acc : 174010.625\n",
            "Epoch 29, loss : 3066352619.7173, val_loss : 578096267264.0000, acc : 173997.062\n",
            "Epoch 30, loss : 3066213453.2934, val_loss : 578067169280.0000, acc : 174018.125\n",
            "Epoch 31, loss : 3065950482.6381, val_loss : 578068283392.0000, acc : 174019.391\n",
            "Epoch 32, loss : 3065484089.0107, val_loss : 577952415744.0000, acc : 174045.406\n",
            "Epoch 33, loss : 3065424032.0685, val_loss : 577944682496.0000, acc : 174049.203\n",
            "Epoch 34, loss : 3065090194.9122, val_loss : 577937604608.0000, acc : 174052.406\n",
            "Epoch 35, loss : 3064808901.3448, val_loss : 577896251392.0000, acc : 174083.406\n",
            "Epoch 36, loss : 3064636957.0535, val_loss : 577898938368.0000, acc : 174088.969\n",
            "Epoch 37, loss : 3064562702.2527, val_loss : 577915518976.0000, acc : 174080.719\n",
            "Epoch 38, loss : 3064548486.3041, val_loss : 577922138112.0000, acc : 174097.766\n",
            "Epoch 39, loss : 3064278956.1285, val_loss : 577866235904.0000, acc : 174117.047\n",
            "Epoch 40, loss : 3064347907.2891, val_loss : 577907458048.0000, acc : 174110.766\n",
            "Epoch 41, loss : 3064307430.2355, val_loss : 577922727936.0000, acc : 174116.047\n",
            "Epoch 42, loss : 3063922945.0964, val_loss : 577940226048.0000, acc : 174124.172\n",
            "Epoch 43, loss : 3063932372.1456, val_loss : 577874821120.0000, acc : 174144.781\n",
            "Epoch 44, loss : 3063781904.4454, val_loss : 577906147328.0000, acc : 174127.828\n",
            "Epoch 45, loss : 3063863612.8480, val_loss : 577920434176.0000, acc : 174137.859\n",
            "Epoch 46, loss : 3063699943.3319, val_loss : 577887600640.0000, acc : 174142.297\n",
            "Epoch 47, loss : 3063620193.0278, val_loss : 577811972096.0000, acc : 174168.109\n",
            "Epoch 48, loss : 3063420473.5589, val_loss : 577850703872.0000, acc : 174154.500\n",
            "Epoch 49, loss : 3063424299.8544, val_loss : 577856339968.0000, acc : 174168.188\n",
            "Epoch 50, loss : 3062877065.0450, val_loss : 577723564032.0000, acc : 174209.594\n",
            "Epoch 51, loss : 3063107823.5546, val_loss : 577782939648.0000, acc : 174187.828\n",
            "Epoch 52, loss : 3063142833.0621, val_loss : 577786675200.0000, acc : 174197.844\n",
            "Epoch 53, loss : 3062866621.1221, val_loss : 577743552512.0000, acc : 174214.844\n",
            "Epoch 54, loss : 3062650391.0236, val_loss : 577654292480.0000, acc : 174240.125\n",
            "Epoch 55, loss : 3062621154.3983, val_loss : 577716617216.0000, acc : 174218.844\n",
            "Epoch 56, loss : 3062538683.4775, val_loss : 577670283264.0000, acc : 174242.375\n",
            "Epoch 57, loss : 3062681575.8801, val_loss : 577653506048.0000, acc : 174250.656\n",
            "Epoch 58, loss : 3062489477.2077, val_loss : 577688043520.0000, acc : 174241.672\n",
            "Epoch 59, loss : 3062366066.0214, val_loss : 577562673152.0000, acc : 174292.750\n",
            "Epoch 60, loss : 3062394698.5525, val_loss : 577623621632.0000, acc : 174259.641\n",
            "Epoch 61, loss : 3062278924.0600, val_loss : 577660518400.0000, acc : 174259.641\n",
            "Epoch 62, loss : 3061717273.7645, val_loss : 577504149504.0000, acc : 174164.375\n",
            "Epoch 63, loss : 3063443795.3233, val_loss : 577635221504.0000, acc : 174303.984\n",
            "Epoch 64, loss : 3062170901.9272, val_loss : 577650950144.0000, acc : 174275.062\n",
            "Epoch 65, loss : 3062532086.1328, val_loss : 577645903872.0000, acc : 174295.062\n",
            "Epoch 66, loss : 3062097547.2377, val_loss : 577553629184.0000, acc : 174319.453\n",
            "Epoch 67, loss : 3061893509.7559, val_loss : 577491828736.0000, acc : 174351.875\n",
            "Epoch 68, loss : 3061707182.8694, val_loss : 577464303616.0000, acc : 174326.469\n",
            "Epoch 69, loss : 3061961595.3405, val_loss : 577462927360.0000, acc : 174331.078\n",
            "Epoch 70, loss : 3061720929.5760, val_loss : 577464107008.0000, acc : 174343.406\n",
            "Epoch 71, loss : 3061780394.4839, val_loss : 577502969856.0000, acc : 174341.859\n",
            "Epoch 72, loss : 3061711186.7752, val_loss : 577500282880.0000, acc : 174346.734\n",
            "Epoch 73, loss : 3061231706.4497, val_loss : 577502576640.0000, acc : 174370.656\n",
            "Epoch 74, loss : 3060896947.2548, val_loss : 577498316800.0000, acc : 174370.688\n",
            "Epoch 75, loss : 3061044277.7216, val_loss : 577440579584.0000, acc : 174371.938\n",
            "Epoch 76, loss : 3061006647.3662, val_loss : 577444511744.0000, acc : 174371.562\n",
            "Epoch 77, loss : 3061047701.6531, val_loss : 577371504640.0000, acc : 174404.469\n",
            "Epoch 78, loss : 3060552856.3940, val_loss : 577254064128.0000, acc : 174420.469\n",
            "Epoch 79, loss : 3060679868.5739, val_loss : 577282899968.0000, acc : 174397.703\n",
            "Epoch 80, loss : 3060677683.5289, val_loss : 577335721984.0000, acc : 174406.250\n",
            "Epoch 81, loss : 3060632825.4218, val_loss : 577269923840.0000, acc : 174442.172\n",
            "Epoch 82, loss : 3060394015.2463, val_loss : 577249476608.0000, acc : 174435.672\n",
            "Epoch 83, loss : 3060660184.5310, val_loss : 577233223680.0000, acc : 174433.203\n",
            "Epoch 84, loss : 3060512791.0236, val_loss : 577260617728.0000, acc : 174428.391\n",
            "Epoch 85, loss : 3060351568.5824, val_loss : 577243054080.0000, acc : 174450.141\n",
            "Epoch 86, loss : 3060234118.3041, val_loss : 577185972224.0000, acc : 174470.703\n",
            "Epoch 87, loss : 3060331941.0021, val_loss : 577281720320.0000, acc : 174444.938\n",
            "Epoch 88, loss : 3060256226.9465, val_loss : 577238073344.0000, acc : 174461.719\n",
            "Epoch 89, loss : 3060097539.8373, val_loss : 577161396224.0000, acc : 174474.953\n",
            "Epoch 90, loss : 3059928135.2634, val_loss : 577157791744.0000, acc : 174474.344\n",
            "Epoch 91, loss : 3060136352.0685, val_loss : 577170767872.0000, acc : 174483.156\n",
            "Epoch 92, loss : 3059824860.3683, val_loss : 577157529600.0000, acc : 174490.172\n",
            "Epoch 93, loss : 3060145264.9251, val_loss : 577225490432.0000, acc : 174478.016\n",
            "Epoch 94, loss : 3060030955.1692, val_loss : 577181253632.0000, acc : 174505.141\n",
            "Epoch 95, loss : 3059966408.0857, val_loss : 577115389952.0000, acc : 174519.344\n",
            "Epoch 96, loss : 3059984733.1906, val_loss : 577149796352.0000, acc : 174505.875\n",
            "Epoch 97, loss : 3059711879.9486, val_loss : 577127186432.0000, acc : 174529.625\n",
            "Epoch 98, loss : 3059812055.4347, val_loss : 577158774784.0000, acc : 174521.047\n",
            "Epoch 99, loss : 3059801331.3919, val_loss : 577128628224.0000, acc : 174541.109\n",
            "test_acc : 171166.812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZeGvJhKe-uZ"
      },
      "source": [
        "#問題5"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhRA3R6UVH-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d5012d-93e1-4725-bacf-32e561744faa"
      },
      "source": [
        "!pip install keras==2.2.4"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\r\u001b[K     |█                               | 10kB 21.7MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 27.5MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 20.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 22.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 17.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 17.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 92kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 16.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 112kB 16.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 16.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 133kB 16.9MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 16.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 16.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 163kB 16.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 16.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 184kB 16.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 16.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 16.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 16.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 225kB 16.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 16.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 245kB 16.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 256kB 16.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 266kB 16.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 276kB 16.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 16.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 296kB 16.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 16.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 16.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-1OdjakV7zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0833c7-f687-4384-dc93-6c8862d5b704"
      },
      "source": [
        "from keras.datasets import mnist\r\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            " 1884160/11490434 [===>..........................] - ETA: 0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEClngaQXKBO"
      },
      "source": [
        "X_train = X_train.reshape(-1, 784)\r\n",
        "X_test = X_test.reshape(-1, 784)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsbYgatzXP_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c8ed67-e654-4231-e2e5-5119099c5fbc"
      },
      "source": [
        "y_test = y_test.astype(np.int64)[:, np.newaxis]\r\n",
        "X_train = np.array(X_train)\r\n",
        "X_test =np.array(X_test)\r\n",
        "y_train = np.array(y_train)\r\n",
        "y_test = np.array(y_test)\r\n",
        "#y_train = y_train.reshape(-1, 1) == np.arange(3)\r\n",
        "y_train = y_train.reshape(-1, 1) == np.arange(10)\r\n",
        "y_test=y_test.reshape(-1, 1) == np.arange(10)\r\n",
        "print(X_test.shape)\r\n",
        "print(y_test.shape)\r\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 784)\n",
            "(10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1hTjTW0ZVep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd15a764-d925-4b37-9b36-013e6d8aeea2"
      },
      "source": [
        "def split_data(data, permutation, val_size_rate=0.2):\r\n",
        "    data = data[permutation]\r\n",
        "    val_size = int(len(data) * val_size_rate)\r\n",
        "    val = data[:val_size]\r\n",
        "    train = data[val_size:]\r\n",
        "    return train, val\r\n",
        "\r\n",
        "permutation = np.random.permutation(np.arange(len(X_train)))\r\n",
        "X_train, X_val = split_data(X_train, permutation)\r\n",
        "y_train, y_val = split_data(y_train, permutation)\r\n",
        "print(X_train.shape)  # (48000, 784)\r\n",
        "print(y_train.shape)  # (12000, 784)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(48000, 784)\n",
            "(48000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96mbnpZxVJDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ce3735-c26a-41c3-c9ee-07e27fa51d20"
      },
      "source": [
        "class GetMiniBatch:\r\n",
        "    \"\"\"\r\n",
        "    ミニバッチを取得するイテレータ\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\r\n",
        "      訓練データ\r\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\r\n",
        "      正解値\r\n",
        "    batch_size : int\r\n",
        "      バッチサイズ\r\n",
        "    seed : int\r\n",
        "      NumPyの乱数のシード\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\r\n",
        "        self.batch_size = batch_size\r\n",
        "        np.random.seed(seed)\r\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\r\n",
        "        self.X = X[shuffle_index]\r\n",
        "        self.y = y[shuffle_index]\r\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\r\n",
        "    def __len__(self):\r\n",
        "        return self._stop\r\n",
        "    def __getitem__(self,item):\r\n",
        "        p0 = item*self.batch_size\r\n",
        "        p1 = item*self.batch_size + self.batch_size\r\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \r\n",
        "    def __iter__(self):\r\n",
        "        self._counter = 0\r\n",
        "        return self\r\n",
        "    def __next__(self):\r\n",
        "        if self._counter >= self._stop:\r\n",
        "            raise StopIteration()\r\n",
        "        p0 = self._counter*self.batch_size\r\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\r\n",
        "        self._counter += 1\r\n",
        "        return self.X[p0:p1], self.y[p0:p1]\r\n",
        "# ハイパーパラメータの設定\r\n",
        "learning_rate = 0.001\r\n",
        "batch_size = 10\r\n",
        "num_epochs = 15\r\n",
        "n_hidden1 = 50\r\n",
        "n_hidden2 = 100\r\n",
        "n_input = X_train.shape[1]\r\n",
        "n_samples = X_train.shape[0]\r\n",
        "n_classes = 10\r\n",
        "# 計算グラフに渡す引数の形を決める\r\n",
        "X = tf.placeholder(\"float\", [None, n_input])\r\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\r\n",
        "# trainのミニバッチイテレータ\r\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\r\n",
        "def example_net(x):\r\n",
        "    \"\"\"\r\n",
        "    単純な3層ニューラルネットワーク\r\n",
        "    \"\"\"\r\n",
        "    tf.random.set_random_seed(0)\r\n",
        "    # 重みとバイアスの宣言\r\n",
        "    weights = {\r\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\r\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\r\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\r\n",
        "    }\r\n",
        "    biases = {\r\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\r\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\r\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\r\n",
        "    }\r\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\r\n",
        "    layer_1 = tf.nn.relu(layer_1)\r\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\r\n",
        "    layer_2 = tf.nn.relu(layer_2)\r\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\r\n",
        "    return layer_output\r\n",
        "# ネットワーク構造の読み込み                               \r\n",
        "logits = example_net(X)\r\n",
        "# 目的関数\r\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\r\n",
        "# 最適化手法\r\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n",
        "train_op = optimizer.minimize(loss_op)\r\n",
        "# 推定結果\r\n",
        "correct_pred = tf.equal(tf.argmax(Y,1 ), tf.argmax(logits,1))\r\n",
        "# 指標値計算\r\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n",
        "# variableの初期化\r\n",
        "init = tf.global_variables_initializer()\r\n",
        "\r\n",
        "# 計算グラフの実行\r\n",
        "with tf.Session() as sess:\r\n",
        "    sess.run(init)\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        # エポックごとにループ\r\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\r\n",
        "        total_loss = 0\r\n",
        "        total_acc = 0\r\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\r\n",
        "            # ミニバッチごとにループ\r\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\r\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\r\n",
        "            total_loss += loss\r\n",
        "        total_loss /= n_samples\r\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\r\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\r\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\r\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss : 700.3600, val_loss : 1987.4568, acc : 0.814\n",
            "Epoch 1, loss : 124.9743, val_loss : 1019.6788, acc : 0.849\n",
            "Epoch 2, loss : 66.3766, val_loss : 717.6204, acc : 0.874\n",
            "Epoch 3, loss : 41.9726, val_loss : 617.4017, acc : 0.874\n",
            "Epoch 4, loss : 29.3925, val_loss : 501.5846, acc : 0.883\n",
            "Epoch 5, loss : 22.2571, val_loss : 457.7954, acc : 0.890\n",
            "Epoch 6, loss : 17.6478, val_loss : 386.3324, acc : 0.898\n",
            "Epoch 7, loss : 14.3525, val_loss : 397.5743, acc : 0.897\n",
            "Epoch 8, loss : 11.9312, val_loss : 330.2914, acc : 0.901\n",
            "Epoch 9, loss : 10.2793, val_loss : 334.1039, acc : 0.905\n",
            "Epoch 10, loss : 9.0249, val_loss : 318.7542, acc : 0.905\n",
            "Epoch 11, loss : 7.7918, val_loss : 294.2523, acc : 0.912\n",
            "Epoch 12, loss : 6.9274, val_loss : 290.9631, acc : 0.910\n",
            "Epoch 13, loss : 6.3413, val_loss : 288.7235, acc : 0.914\n",
            "Epoch 14, loss : 5.8109, val_loss : 281.3247, acc : 0.916\n",
            "test_acc : 0.919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTIiaOWcYzbi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}